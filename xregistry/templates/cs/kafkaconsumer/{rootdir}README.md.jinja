{%- import "util.jinja.include" as util -%}
{%- import "cloudevents.jinja.include" as cloudEvents -%}
{%- set messagegroups = root.messagegroups %}
{% for messagegroupid, messagegroup in messagegroups.items() -%}
{%- set groupname = messagegroupid | pascal -%}
{%- set class_name = (groupname | strip_namespace) + "EventConsumer" %}
# {{ project_name | pascal }} - Apache Kafka Consumer

Auto-generated Apache Kafka consumer for {{ groupname }} message group.

## Overview

This consumer library makes it simple to receive and process messages from Apache Kafka topics. It handles connection management, deserialization, and error recovery so you can focus on your business logic.

## What is Apache Kafka?

Apache Kafka is a distributed event streaming platform used for:
- Building real-time data pipelines
- Streaming analytics
- Event-driven microservices
- Log aggregation

This library lets you consume Kafka messages without dealing with the low-level Kafka consumer API.

## Quick Start

### 1. Basic Usage

```csharp
using {{ project_name | pascal }};
using Microsoft.Extensions.Logging;

// Create consumer
var consumer = new {{ class_name }}(
    "localhost:9092",           // Bootstrap servers
    "my-consumer-group",         // Consumer group
    new[] { "my-topic" },        // Topics to subscribe
    loggerFactory);

// Register event handlers
{%- set first_message = messagegroup.messages.items() | first %}
{%- if first_message %}
{%- set messageid, message = first_message %}
{%- set messagename = messageid | strip_dots | pascal %}
{%- set message_body_type = util.body_type(data_project_name, root, message) %}
consumer.{{ messagename }}Async += async (record, cloudEvent, data) =>
{
    Console.WriteLine($"Received: {cloudEvent.Id}");
    await ProcessAsync(data);
};
{%- endif %}

// Start consuming
await consumer.StartAsync();

// Keep running
await Task.Delay(Timeout.Infinite, cancellationToken);

// Cleanup
await consumer.StopAsync();
```

### 2. Configuration Options

```csharp
var config = new Dictionary<string, string>
{
    ["bootstrap.servers"] = "localhost:9092",
    ["group.id"] = "my-consumer-group",
    ["auto.offset.reset"] = "earliest",  // Start from beginning
    ["enable.auto.commit"] = "true"      // Auto-commit offsets
};

var consumer = {{ class_name }}.CreateConsumer(
    config,
    new[] { "my-topic" },
    loggerFactory);
```

## Event Handlers

{% for messageid, message in messagegroup.messages.items() -%}
{%- set messagename = messageid | strip_dots | pascal %}
{%- set message_body_type = util.body_type(data_project_name, root, message) %}
### {{ messagename }}

**Event Type:** `{{ messageid }}`  
**Data Type:** `{{ message_body_type }}`

```csharp
consumer.{{ messagename }}Async += async (record, cloudEvent, data) =>
{
    // record: Kafka ConsumerRecord with metadata
    // cloudEvent: CloudEvents envelope
    // data: Strongly-typed message data
    
    _logger.LogInformation("Processing from partition {Partition}, offset {Offset}",
        record.Partition, record.Offset);
    
    await ProcessDataAsync(data);
};
```

{% endfor %}

## Consumer Groups

Consumer groups enable parallel processing. All consumers in the same group share the workload.

```csharp
// Instance 1 (e.g., server-1)
var consumer1 = new {{ class_name }}("localhost:9092", "my-group", topics, logger);

// Instance 2 (e.g., server-2)  
var consumer2 = new {{ class_name }}("localhost:9092", "my-group", topics, logger);

// Kafka automatically distributes partitions between them!
```

**Different groups** process independently:
```csharp
// Analytics consumer (processes all messages)
var analyticsConsumer = new {{ class_name }}("localhost:9092", "analytics-group", topics, logger);

// Storage consumer (also processes all messages independently)
var storageConsumer = new {{ class_name }}("localhost:9092", "storage-group", topics, logger);
```

## Authentication

### SASL/PLAIN (Username/Password)

```csharp
var config = new Dictionary<string, string>
{
    ["bootstrap.servers"] = "kafka.example.com:9093",
    ["security.protocol"] = "SASL_SSL",
    ["sasl.mechanism"] = "PLAIN",
    ["sasl.username"] = "your-username",
    ["sasl.password"] = "your-password"
};
```

### SASL/SCRAM (More Secure)

```csharp
var config = new Dictionary<string, string>
{
    ["bootstrap.servers"] = "kafka.example.com:9093",
    ["security.protocol"] = "SASL_SSL",
    ["sasl.mechanism"] = "SCRAM-SHA-256",
    ["sasl.username"] = "your-username",
    ["sasl.password"] = "your-password"
};
```

### SSL/TLS with Certificates

```csharp
var config = new Dictionary<string, string>
{
    ["bootstrap.servers"] = "kafka.example.com:9093",
    ["security.protocol"] = "SSL",
    ["ssl.ca.location"] = "/path/to/ca-cert",
    ["ssl.certificate.location"] = "/path/to/client-cert",
    ["ssl.key.location"] = "/path/to/client-key"
};
```

## Offset Management

**Offsets** track which messages have been processed.

### Auto-commit (Default)

```csharp
var config = new Dictionary<string, string>
{
    ["enable.auto.commit"] = "true",
    ["auto.commit.interval.ms"] = "5000"  // Commit every 5 seconds
};
```

Messages are automatically marked as processed. Simple but less control.

### Manual Commit (More Control)

```csharp
var config = new Dictionary<string, string>
{
    ["enable.auto.commit"] = "false"
};

consumer.{{ messagegroup.messages.keys() | first | strip_dots | pascal }}Async += async (record, cloudEvent, data) =>
{
    try
    {
        await ProcessAsync(data);
        await consumer.CommitAsync(record); // Explicit commit
    }
    catch (Exception ex)
    {
        _logger.LogError(ex, "Processing failed, will retry");
        // Don't commit - message will be reprocessed
    }
};
```

### Offset Reset Strategy

```csharp
// Start from earliest available message
["auto.offset.reset"] = "earliest"

// Start from latest (skip old messages)
["auto.offset.reset"] = "latest"
```

## Error Handling

```csharp
consumer.ProcessingErrorAsync += async (record, cloudEvent, exception) =>
{
    _logger.LogError(exception, "Failed processing offset {Offset}", record.Offset);
    
    // Send to dead-letter topic
    await _deadLetterProducer.ProduceAsync("errors", cloudEvent);
};

consumer.DeserializationErrorAsync += async (record, cloudEvent, exception) =>
{
    _logger.LogWarning(exception, "Bad message at offset {Offset}", record.Offset);
    // Message is skipped
};
```

## Testing

```bash
dotnet test
```

Sample test:
```csharp
[Fact]
public async Task ConsumesKafkaMessage()
{
    // Arrange: Start Kafka container
    using var kafka = new KafkaContainer();
    await kafka.StartAsync();
    
    var consumer = new {{ class_name }}(
        kafka.BootstrapServers,
        "test-group",
        new[] { "test-topic" },
        loggerFactory);
    
    var received = new TaskCompletionSource<bool>();
    consumer.{{ messagegroup.messages.keys() | first | strip_dots | pascal }}Async += async (r, ce, data) =>
    {
        received.SetResult(true);
    };
    
    await consumer.StartAsync();
    
    // Act: Produce test message
    await ProduceTestMessageAsync(kafka);
    
    // Assert
    Assert.True(await received.Task.WaitAsync(TimeSpan.FromSeconds(10)));
    await consumer.StopAsync();
}
```

## Performance Tips

1. **Batch processing**: Process multiple messages before committing
2. **Parallel handlers**: Use Task.WhenAll for independent operations
3. **Optimize serialization**: Use efficient formats (Avro, Protobuf)
4. **Increase partitions**: More partitions = more parallel consumers
5. **Tune fetch settings**: Adjust `fetch.min.bytes` and `fetch.max.wait.ms`

## Configuration Reference

Common Kafka consumer settings:

| Setting | Description | Example |
|---------|-------------|---------|
| `bootstrap.servers` | Kafka brokers | `"localhost:9092"` |
| `group.id` | Consumer group | `"my-app"` |
| `auto.offset.reset` | Where to start | `"earliest"` or `"latest"` |
| `enable.auto.commit` | Auto-commit offsets | `"true"` or `"false"` |
| `max.poll.records` | Max messages per poll | `"500"` |
| `session.timeout.ms` | Consumer heartbeat timeout | `"30000"` |

## Learn More

- [Apache Kafka Documentation](https://kafka.apache.org/documentation/)
- [Confluent Kafka .NET Client](https://docs.confluent.io/kafka-clients/dotnet/current/overview.html)
- [CloudEvents Specification](https://cloudevents.io/)
- [xRegistry CLI Documentation](https://github.com/clemensv/xregistry-cli)

## Dependencies

- `Confluent.Kafka` - Kafka .NET client
- `CloudNative.CloudEvents` - CloudEvents support
- `Microsoft.Extensions.Logging` - Logging

## Generated Code

This code was auto-generated by [xRegistry CLI](https://github.com/clemensv/xregistry-cli).

**Message Group:** {{ groupname }}  
**Protocol:** Apache Kafka  
**Envelope:** CloudEvents 1.0
{% endfor %}
