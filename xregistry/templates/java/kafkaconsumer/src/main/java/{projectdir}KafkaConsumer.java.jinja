{%- import 'util.jinja.include' as util -%}
{{ util.CommonFileHeader() }}

{%- set package_name = project_name | lower | replace('-', '_') %}
package {{ package_name }};

import org.apache.kafka.clients.consumer.Consumer;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.common.serialization.ByteArrayDeserializer;
import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import java.net.URI;
import java.time.Duration;
import java.util.*;
import java.util.concurrent.atomic.AtomicBoolean;

{%- for messagegroupid in root.messagegroups.keys() %}
{%- set group_package = (package_name ~ "." ~ messagegroupid) | lower | replace('-', '_') -%}
import {{ group_package }}.*;
{%- endfor %}

/**
 * Kafka Consumer for {{ project_name }}.
 * Manages Kafka consumer lifecycle and provides factory methods for creating consumers.
 */
public class {{ project_name | pascal }}KafkaConsumer implements AutoCloseable {
    private static final Logger logger = LogManager.getLogger({{ project_name | pascal }}KafkaConsumer.class);
    
    private final Consumer<byte[], byte[]> consumer;
    private final AtomicBoolean running = new AtomicBoolean(false);
    private Thread consumerThread;

    /**
     * Creates a new instance of {{ project_name | pascal }}KafkaConsumer.
     * 
     * @param consumer The Kafka consumer to use for consuming events
     */
    public {{ project_name | pascal }}KafkaConsumer(Consumer<byte[], byte[]> consumer) {
        this.consumer = consumer;
    }

    {%- for messagegroupid, messagegroup in root.messagegroups.items() %}
    {%- set messagegroupname = messagegroupid | pascal | strip_namespace %}
    {%- set group_package = (package_name ~ "." ~ messagegroupid) | lower | replace('-', '_') %}
    {%- set consumer_class = messagegroupname ~ 'EventConsumer' %}

    /**
     * Create a Kafka consumer for {{ messagegroupid }}.
     * 
     * @param eventConsumer The event consumer that handles incoming messages
     * @param bootstrapServers List of Kafka bootstrap server URIs
     * @param groupId Consumer group ID
     * @param topics List of topics to subscribe to
     * @return A configured consumer instance
     */
    public static {{ project_name | pascal }}KafkaConsumer createFor{{ messagegroupname }}(
            {{ group_package }}.{{ consumer_class }} eventConsumer,
            List<URI> bootstrapServers,
            String groupId,
            List<String> topics) {
        
        Properties props = new Properties();
        
        // Build bootstrap servers string
        StringBuilder bootstrapServersStr = new StringBuilder();
        for (int i = 0; i < bootstrapServers.size(); i++) {
            URI uri = bootstrapServers.get(i);
            String host = uri.getHost() != null ? uri.getHost() : "localhost";
            int port = uri.getPort() > 0 ? uri.getPort() : 9092;
            
            if (i > 0) {
                bootstrapServersStr.append(",");
            }
            bootstrapServersStr.append(host).append(":").append(port);
        }
        
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServersStr.toString());
        props.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "true");
        props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, "1000");
        
        org.apache.kafka.clients.consumer.KafkaConsumer<byte[], byte[]> consumer = new org.apache.kafka.clients.consumer.KafkaConsumer<>(props);
        consumer.subscribe(topics);
        
        {{ project_name | pascal }}KafkaConsumer kafkaConsumer = new {{ project_name | pascal }}KafkaConsumer(consumer);
        kafkaConsumer.setEventConsumer(eventConsumer);
        
        logger.info("Created Kafka consumer for group {} subscribed to topics {}", groupId, topics);
        return kafkaConsumer;
    }
    {%- endfor %}

    private Object eventConsumer;
    
    private void setEventConsumer(Object eventConsumer) {
        this.eventConsumer = eventConsumer;
    }

    /**
     * Start consuming messages from Kafka.
     * This method returns immediately; processing happens in a background thread.
     */
    public void start() {
        if (running.compareAndSet(false, true)) {
            logger.info("Starting Kafka consumer");
            consumerThread = new Thread(() -> {
                try {
                    while (running.get()) {
                        ConsumerRecords<byte[], byte[]> records = consumer.poll(Duration.ofMillis(100));
                        if (!records.isEmpty()) {
                            {%- for messagegroupid, messagegroup in root.messagegroups.items() %}
                            {%- set messagegroupname = messagegroupid | pascal | strip_namespace %}
                            {%- set group_package = (package_name ~ "." ~ messagegroupid) | lower | replace('-', '_') %}
                            {%- set consumer_class = messagegroupname ~ 'EventConsumer' %}
                            if (eventConsumer instanceof {{ group_package }}.{{ consumer_class }}) {
                                (({{ group_package }}.{{ consumer_class }}) eventConsumer).processRecords(records);
                            }
                            {%- endfor %}
                        }
                    }
                } catch (Exception e) {
                    if (running.get()) {
                        logger.error("Error in consumer loop", e);
                    }
                } finally {
                    logger.info("Consumer loop stopped");
                }
            });
            consumerThread.setName("kafka-consumer-thread");
            consumerThread.setDaemon(false);
            consumerThread.start();
        }
    }

    /**
     * Stop consuming messages and release resources.
     */
    @Override
    public void close() {
        logger.info("Stopping Kafka consumer");
        running.set(false);
        
        if (consumerThread != null) {
            try {
                consumerThread.join(5000);
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
                logger.warn("Interrupted while waiting for consumer thread to stop");
            }
        }
        
        if (consumer != null) {
            consumer.close();
        }
    }
}
